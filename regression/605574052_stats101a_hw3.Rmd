---
title: "Homework 3"
author: "Jun Ryu, UID: 605574052"
output: pdf_document
date: "2023-04-21"
---

```{r}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
```

## Question 1

### a)

```{r}
linear_model <- function(beta_0, beta_1, sigma, x, random.seed = 123) {
  set.seed(random.seed)
  epsilon <- rnorm(length(x), 0, sigma)
  beta_0 + beta_1*x + epsilon
}

input_x <- rep(1:10,by=.1,4)
plot(input_x, linear_model(1, 3, 3, input_x, 123))
```

### b)

```{r}
cor(input_x, linear_model(1, 3, 3, input_x, 123))
```

### c)

```{r}
# only need to change the sigma value from 3 to 1 to minimize the errors
linear_model(1, 3, 1, input_x, 123)
cor(input_x, linear_model(1, 3, 1, input_x, 123))
```

## Question 2

```{r}
armspan22 <- read.csv("armspans2022_gender.csv")
armspan22 <- na.omit(armspan22) # remove the observations with NA values
head(armspan22)
```

### a)

```{r}
plot(armspan~height, data=armspan22)
cor(armspan22$height, armspan22$armspan)
```

According to the plot, the two variables seem to have a strong positive correlation. Indeed, checking the correlation coefficient, we get 0.92147 (a value very close to 1), indicating a strong positive correlation. 

### b)

```{r}
model <- lm(armspan~height, data=armspan22)
model # yields the equation: armspan = 1.425*height - 29.635
plot(armspan~height, data=armspan22)
abline(model)
```

### c)

```{r}
# for my height 5'7'' (67 inches):
predict(model, data.frame(height = 67)) # prediction: 65.8 inches
# my actual armspan is 68 inches
residual <- 68 - predict(model, data.frame(height = 67))
residual # (actual - predicted) = 2.188 inches
```

### d)

```{r}
# for Michael Phelps (76 inches tall):
predict(model, data.frame(height = 76))
```

From the linear model and the corresponding prediction of 78.63 inches, his armspan length (79 inches) is not unusual.

### e)

```{r}
plot(armspan22$height, resid(model))
abline(0,0)
```

Looking at the residual plot, we see a good constant spread of the residual up and down the midline (indicating 0 error) across the independent observations. Therefore, this indicates a good linear fit between the two variables of armspan and height.

## Question 3

### a)

```{r}
quadratic_model <- function(a, b, c, sigma, x, random.seed = 123) {
  set.seed(random.seed)
  epsilon <- rnorm(length(x), 0, sigma)
  a + b*x + c*x^2 + epsilon
}

input_x <- rep(1:10,by=.1,4)
plot(input_x, quadratic_model(2, 2, 3, 5, input_x, 123))
```

### b)

```{r}
model2 <- lm(quadratic_model(2, 2, 3, 5, input_x, 123) ~ input_x)
model2

plot(input_x, resid(model2))
abline(0,0)
```

From the above residual plot, we observe that the residuals are not constantly spread across the x variable. For example, in the region where the x values are $[4,8]$, the residuals are all negative, while in the region outside of that bound, the residuals are mostly positive.

### c)

We can use the residual plot to see if the residuals show a nature of randomness (independent, normal, and constant standard deviation). If the residuals seem to violate one of these three properties, then we can tell the trend is probably non-linear.

### d)

```{r}
revised_linear_model <- function(a, b, sigma, x, random.seed = 123) {
  set.seed(random.seed)
  epsilon <- rnorm(length(x), 0, sigma*x^2)
  a + b*x + epsilon
}

input_x <- rep(1:10,by=.1,4)
plot(input_x, revised_linear_model(1, 200, 5, input_x, 123))
```

The plot does not seem to show a clear linear trend as the deviations cause some of the points to be further away from the linear fit.

### e)

```{r}
model3 <- lm(revised_linear_model(1, 200, 5, input_x, 123) ~ input_x)
model3

plot(input_x, resid(model3)) 
abline(0,0)
```

We notice that as the x values get larger, the residuals also become more extreme (in both positive and negative directions). This violates the constant standard deviation part as the residuals get further and further away.

## Question 4

### a)

```{r}
atus <- read.csv("atus.csv")
atus1 <- subset(atus,homework>0)

plot(homework ~ sleep, data=atus1)
model4 <- lm(homework ~ sleep, data=atus1)
model4
abline(model4)
```

The linear fit shows a negative correlation between the amount of time spent on sleep vs. homework, but looking at the plot in general, the model does not seem to be a good fit as we have numerous points that are widely detached from the linear model.

### b)

```{r}
plot(atus1$sleep, resid(model4))
abline(0,0)
```

From the residual plot, we can deduce a similar observation where a lot of the residuals are way above the midline, indicating a non-normal distribution of deviations. Thus, this linear model is not a great fit for this dataset.

## Question 5

### a)

Let $\mu_1 =$ (Average time doing household chores for those who identified as female) and $\mu_2 =$ (Average time doing household chores for those who identified as male). Our null is $H_0: \mu_1 = \mu_2$ and our alternative is $H_a: \mu_1 \neq \mu_2$.

```{r}
t.test(atus1$household_chores[atus1$gender == "Female"], 
       atus1$household_chores[atus1$gender == "Male"])
```

The test statistic here is the t-test value, which is 6.3978. The p-value is $3.986^{-10}$ assuming unequal variance. Using a 5% significance level, since our p-value is less than 0.05, we have sufficient evidence to reject the null hypothesis and conclude that there is a significant difference between the average time doing household chores for female vs. male.

### b)

Some conditions that must be met include normality of population, independence between observations, random sampling, and homogeneity of variance. In our dataset, these are satisfied because we have a random sample with a sufficient sample size and independent observations.



































