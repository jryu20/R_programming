---
title: "Homework 10"
author: 'Jun Ryu, UID: 605574052'
date: "2023-06-09"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(MASS)
library(car)
library(leaps)
```

## Textbook Exercise 6.5

```{r}
pga <- read.csv("pgatour2006-3.csv")
head(pga)
```

### a)

```{r}
model <- lm(PrizeMoney ~ DrivingAccuracy+GIR+PuttingAverage+BirdieConversion+
              SandSaves+Scrambling+PuttsPerRound, data = pga)
invResPlot(model)
summary(powerTransform(model))
```

Looking at the inverse response plot, the plot suggests that a log transform would be appropriate as its plot using the "optimal" value of 0.12 is not too different from the log transform graph (lambda = 0). Moreover, checking with the power transform, because the p-value is high for the hypothesis test when lambda = 0, we do not reject the null hypothesis and conclude that a log transform of the Y variable would be appropriate.

### b)

```{r}
# using a log transform...
log_model <- lm(log(PrizeMoney) ~ DrivingAccuracy+GIR+PuttingAverage+BirdieConversion+
              SandSaves+Scrambling+PuttsPerRound, data = pga)
summary(log_model)
plot(log_model)
```

This improved model seems to be valid after analyzing the diagnostic plots. There's no visible trend in the residuals vs fitted and the scale-location plot, indicating linearity and constant variance. The qq-plot is also pretty straight, indicating normality of our model.

### c)

Some points that should be investigated include outliers. In our case, observation 185 seems to be a potential outlier because not only does it have the highest residual in the residuals vs fitted plot, but the point seems to be a bad leverage point based on the residuals vs leverage plot. On the same note, observations 47 and 63 could be investigated due to their high residual values.

### d)

A weakness of our model is collinearity. Observing the vif values:

```{r}
vif(log_model)
```

We have that 3 of these variables have a vif value greater than 5, which is problematic. This will decrease t-statistics and thus, inflate the respective p-values.  

### e)

First of all, due to the issue with collinearity, we have to keep in mind that some of these p-values are inflated and thus, the t-statistics are not accurately represented. But, even beyond that, trying to remove all predictors in a single step is not a good idea as removing one variable could actually change another variable's t-statistic, perhaps making it go from insignificant to significant.

## Textbook Exercise 7.3

### a)

```{r}
bestss <- regsubsets(log(PrizeMoney) ~ DrivingAccuracy+GIR+PuttingAverage+BirdieConversion+
                          SandSaves+Scrambling+PuttsPerRound, data = pga, nvmax = 7)
summary(bestss)

# first we use the bic values:
bic <- summary(bestss)$bic
plot(1:7, bic)
lines(1:7, bic)
```

The 3-variable model has the lowest BIC, which is the model that includes the variables GIR, BirdieConversion, and Scrambling. Now we will manually compute the AIC values for comparison:

```{r}
m1 <- AIC(lm(log(PrizeMoney) ~ GIR, data = pga))
m2 <- AIC(lm(log(PrizeMoney) ~ GIR+PuttsPerRound, data = pga))
m3 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion+Scrambling, data = pga))
m4 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion+SandSaves+Scrambling, data = pga))
m5 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion+SandSaves+Scrambling+PuttsPerRound, data = pga))
m6 <- AIC(lm(log(PrizeMoney) ~ DrivingAccuracy+GIR+BirdieConversion+SandSaves+
           Scrambling+PuttsPerRound, data = pga))
m7 <- AIC(log_model)

plot(1:7, c(m1,m2,m3,m4,m5,m6,m7), ylab = "aic")
lines(1:7, c(m1,m2,m3,m4,m5,m6,m7))
```

The 3, 4, 5-variable model seem awfully close in their AIC values.

```{r}
c(m3,m4,m5)
```

We see that the 5-variable model has the lowest AIC values, which is the model that includes the variables GIR, BirdieConversion, SandSaves, Scrambling, and PuttsPerRound.

### b)

```{r}
# using backward selection:
bestss <- regsubsets(log(PrizeMoney) ~ DrivingAccuracy+GIR+PuttingAverage+BirdieConversion+
                          SandSaves+Scrambling+PuttsPerRound, data = pga, nvmax = 7,
                     method = "backward")
summary(bestss)

bic <- summary(bestss)$bic
plot(1:7, bic)
lines(1:7, bic)
```

The 3-variable model has the lowest BIC, the same model as the one in part a). Now the AIC values for backward selection:

```{r}
m1 <- AIC(lm(log(PrizeMoney) ~ GIR, data = pga))
m2 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion, data = pga))
m3 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion+Scrambling, data = pga))
m4 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion+SandSaves+Scrambling, data = pga))
m5 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion+SandSaves+Scrambling+PuttsPerRound, data = pga))
m6 <- AIC(lm(log(PrizeMoney) ~ DrivingAccuracy+GIR+BirdieConversion+SandSaves+
           Scrambling+PuttsPerRound, data = pga))
m7 <- AIC(log_model)

plot(1:7, c(m1,m2,m3,m4,m5,m6,m7), ylab = "aic")
lines(1:7, c(m1,m2,m3,m4,m5,m6,m7))
```

Again, we observe the same result as in part a). Since the variables we used for model 3, 4 and 5 did not change with backward selection, model 5 will still have the lowest AIC value.

### c)

```{r}
# using forward selection:
bestss <- regsubsets(log(PrizeMoney) ~ DrivingAccuracy+GIR+PuttingAverage+BirdieConversion+
                          SandSaves+Scrambling+PuttsPerRound, data = pga, nvmax = 7,
                     method = "forward")
summary(bestss)

bic <- summary(bestss)$bic
plot(1:7, bic)
lines(1:7, bic)
```

Now, the 4-variable model has the lowest BIC value. This model includes the variables GIR, BirdieConversion, Scrambling, and PuttsPerRound.

```{r}
m1 <- AIC(lm(log(PrizeMoney) ~ GIR, data = pga))
m2 <- AIC(lm(log(PrizeMoney) ~ GIR+PuttsPerRound, data = pga))
m3 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion+PuttsPerRound, data = pga))
m4 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion+Scrambling+PuttsPerRound, data = pga))
m5 <- AIC(lm(log(PrizeMoney) ~ GIR+BirdieConversion+SandSaves+Scrambling+PuttsPerRound, data = pga))
m6 <- AIC(lm(log(PrizeMoney) ~ DrivingAccuracy+GIR+BirdieConversion+SandSaves+
           Scrambling+PuttsPerRound, data = pga))
m7 <- AIC(log_model)

plot(1:7, c(m1,m2,m3,m4,m5,m6,m7), ylab = "aic")
lines(1:7, c(m1,m2,m3,m4,m5,m6,m7))
c(m3,m4,m5)
```

Here, the same 5-variable model from the previous two parts has the lowest AIC value.

### d)

Professor told us that we could skip this portion.

### e)

The final recommended model is the 5-variable model with variables GIR, BirdieConversion, SandSaves, Scrambling, and PuttsPerRound because this model had the lowest AIC value across all three methods (exhaustive, backward, and forward).

### f)

```{r}
rec_model <- lm(log(PrizeMoney) ~ GIR+BirdieConversion+SandSaves+Scrambling+PuttsPerRound, data = pga)
summary(rec_model)
```

Here, the intercept represents the average prize value when all other coefficients are fixed to 0. For the slope estimates for each variable, they represent the average increase/decrease in percentage of the prize value when that particular variable is increased by one. It is important to be cautious when taking these results literally because this model's adjusted R-squared value is 0.5459, which means there is still a considerable amount of variation left to be explained. 















